```{r chapter-2-setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
library(PerformanceAnalytics)
library(data.table)
library(tidyverse)
library(hrbrthemes)
library(ggcorrplot)
library(GoodmanKruskal)
library(lindia)
library(finalfit)
library(sigr)
library(papaja)
library(here)
library(kableExtra)
library(GGally)
```

# Week 2 â€“ Regression and model validation

```{r load-data, echo = FALSE, message= FALSE, cache = TRUE}

learning2014 <- fread(here("data/learning2014.csv"))

```

The following analysis uses dataset (n=`r dim(learning2014)[1]`) gathered using *ASSIST: the appproaches and study skills inventory for studets * -survey. For more information regarding the dataset, please see @vehkalahti2016. 

The purpose of the analysis is to explore, whether is it possible to predict student's test performance using composite variables depicting student's mental inclination towards studying, namely: 

1. positive attitude
2. desire to learn deeply
2. desire to learn quickly without commintment to truly understand the subject matter.
4. strategic mentality torwards studying

These four composite variables were created from the set of 42 variables. Original variables were measured using five-point Likert scale and composite variables were scaled back to the five-point range.


## Exploratory analysis {#explorative-analysis}



```{r graphical-overview, cache = TRUE, echo = FALSE, fig.width = 12, fig.height = 4, message = FALSE}
# Show a graphical overview of the data and show summaries of the variables 
# in the data. Describe and interpret the outputs, 
# commenting on the distributions of the variables 
# and the relationships between them. (0-3 points)

ggpairs(
  learning2014,
  mapping = aes(col = gender, alpha = 0.3), 
        lower = list(combo = wrap("facethist", bins = 5))) + theme_minimal()
```

From the explatory graph we can see that the gender and age distribution is rather skewed in the data. There are almost 50% more female students than males -- especially among the students in their twenties. Even though the mean age is `r round(mean(learning2014$age),2)`, there are lots of older students too, the maximum age being `r max(learning2014$age)`. 

Visual inspection indicates that even though there are slight differences in the attitude and learning approch scores (deep, surface, strategic) between males and females it seems that these differences do not have an effect on median test points. From the figure below we can see that the bulk of the data sits right in the middle of the scale so that there are very few datapoints in the ends of the scales.



### Relationship of gender, attitude and strategic study approach

```{r graphical-overview-relationship, cache = TRUE, echo = FALSE}
ggplot(learning2014, aes(color = gender, x = attitude, y = points, size = stra)) + 
  geom_point() + 
  geom_smooth(method = "lm", fill = NA) +
  scale_size_continuous(range = c(0.05,2)) +
  theme_bw()
```

## Regression analysis

In building linear regression models, it is desirable to have high correlations between the prediction covariates and the response variable, but small correlations between the different prediction covariates. Large correlations between prediction variables leads to the [problem of collinearity in linear regression](https://en.wikipedia.org/wiki/Multicollinearity#Consequences_of_multicollinearity), eg. the model might overfit the data. 

Based on explorative analysis in [Fig 1](#explorative-analysis), three variables were chosen as prediction covariates.

For the first model, the following variables were chosen as predictors.

* attitude (correlation with points, 0.437)
* strategic learning approach (correlation with points, 0.146)
* surface learning approach (correlation with points, -0.144)

Attitude and strategic learning approach do not correlate (0.06), strategic learning and surface learning approach correlate slightly (-0.16) as well as surfarce learning approach and attitude (-0.17). 

### Initial model

```{r initial-model-instruction, echo=FALSE, eval=FALSE}
# Choose three variables as explanatory variables and 
# fit a regression model where exam points is the target (dependent) variable.
```


```{r fit-attitude_stra_surf, cache = TRUE}
model_attitude_stra_surf <- lm(
  data = learning2014, 
  formula = points ~ attitude + stra + surf )

```

```{r model_attitude_stra_surf_summary, echo = FALSE}

# Show a summary of the fitted model and comment and interpret the results. 
# Explain and interpret the statistical test related to the model parameters.

summary_colnames <- c(
  "Predictor",
  "Estimate",
  "Confidence Interval (95%)",
  "t-statistic",
  "p-value")

model_attitude_stra_surf_summary <- apa_print(model_attitude_stra_surf)
knitr::kable(
  model_attitude_stra_surf_summary$table,
  format = "html", col.names = summary_colnames, caption = "Regression summary") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

`r render(wrapFTest(model_attitude_stra_surf, pSmallCutoff=0, format="markdown"))` Adjusted <i>R^2^</i> is `r round(summary(model_attitude_stra_surf)$adj.r.squared, 2)`. 

R^2^ is the percentage of the dependent variable variation explained by the linear model. High R-squared values represent smaller differences between the observed data and the fitted values. Having R^2^ = 0.2074, our first model explains 20% of the observed variation in student's test scores.

From the summary we can see, that the surface learning variable (surf) doesn't have statistically significant relationship with test points (p=0.446). Neither Strategic learning approach -variable (stra) is  significant (p=0.177) so we'll exclude them from the final model.


### Final model

```{r final-model-instruction, echo = FALSE, eval = FALSE}
# If an explanatory variable in your model does not have 
# a statistically significant relationship with the target variable
# remove the variable from the model and fit the model again without it.
```

```{r fit-attitude_stra, cache = FALSE}
model_attitude_stra <- lm(
  data = learning2014, 
  formula = points ~ attitude)
```

```{r model_model_attitude_stra_summary, echo = FALSE, cache = FALSE}
model_attitude_stra_summary <- apa_print(model_attitude_stra)
knitr::kable(
  model_attitude_stra_summary$table,
  format="html", col.names = summary_colnames, caption = "Regression summary") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```


For the final model `r render(wrapFTest(model_attitude_stra, pSmallCutoff=0, format="markdown"))` Adjusted <i>R^2^</i> is `r round(summary(model_attitude_stra)$adj.r.squared, 2)`.

We see that R^2^ stays the same for the revised model. Based on the model coefficients (estimate) and 95% confidence interval for the estiate, we see that for the one unit increase in attitude (predictor) variable, the increase in points is between 2.41 and 4.65, average increase being 3.53 points.

## Regression diagnostics

Regression diagnostics plots help to observe, if the model meets the assumptions of linear regression:

* Linearity of residuals
* Independence of residuals
* Normal distribution of residuals
* Equal variance of residuals

```{r instructions, echo = FALSE, eval = FALSE, cache = TRUE}
# Produce the following diagnostic plots: 
# Residuals vs Fitted values
# Normal QQ-plot
# Residuals vs Leverage. 
# Explain the assumptions of the model and interpret the validity of 
# those assumptions based on the diagnostic plots. (0-3 points)
```




```{r residuals-fitted, echo = FALSE, cache = TRUE}
gg_resfitted(model_attitude_stra, scale.factor = 1) + theme_bw()
```


Residuals vs Fitted -plot shows if residuals have non-linear patterns. If the residuals have constant variance of errors, the plot scatters randomly without any distict patterns. We don't see patterns here, so we can conclude that the variance is constant and the assumption of equal variance of residuals is met.


```{r qqplot, echo = FALSE, cache = TRUE}
gg_qqplot(model_attitude_stra, scale.factor = 1) + theme_bw()

```

Normal Q-Q -plot shows if residuals are normally distributed. If the residuals deviate from the straight line it is an indication of non-normality. There are few outlying values in both ends of the line but majority of of the points line nicely and therefore we can conclude that the the residuals of the model are normally distributed.



```{r residual-leverage, echo = FALSE, cache = TRUE}
gg_resleverage(model_attitude_stra, method = "loess", se = FALSE, scale.factor = 1) + theme_bw()

```

Residual vs. leverage -plot helps to find influential cases. Not all outliers have leverage in linear regression analysis. Values to observe are at the upper right corner or at the lower right corner -- they are the ones which can have an effect on regression results. We don't see outlying values in these critical spots, so we can conclude that there is no influencial individual cases in the data.


## Extra - tweaking the model

Even though we just concluded that there's no influencial individual cases in the data, we can see from the plots that there are few outlying cases in non-critical spots of the Residual vs. leverage plot. What happens if we exclude them from the data? To do it, we have to single out the cases from the data using Cook's distance.

```{r cooks-distance, echo = FALSE, cache = FALSE}
gg_cooksd(
  model_attitude_stra,
  scale.factor = 1, show.threshold = FALSE) + 
  theme_bw()
```

Just for the sake of curiosity, let's investige how excluding three outlier cases (36, 56 & 71) has an effect on our regression model. 

```{r fit-attitude_stra-excluded-cases, cache = FALSE}
learning2014$id <- 1:dim(learning2014)[1]
model_attitude_stra_excluded <- lm(
  data = learning2014 %>% filter(!id %in% c(35, 56, 71)), 
  formula = points ~ attitude)
```

```{r model_model_attitude_stra_excluded-summary, echo = FALSE, cache = FALSE}
model_attitude_stra_summary_excluded <- apa_print(model_attitude_stra_excluded)
knitr::kable(
  model_attitude_stra_summary_excluded$table,
  format="html", col.names = summary_colnames, caption = "Regression summary") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```


For the model created using data without cases 35, 56, and 71, the `r render(wrapFTest(model_attitude_stra_excluded, pSmallCutoff=0, format="markdown"))` Adjusted <i>R^2^</i> is `r round(summary(model_attitude_stra_excluded)$adj.r.squared, 2)`.

The fit of the model seems to be marginally better when three outlying cases are excluded. Let's see from the data what kind of cases these are so that we could decide if it is prudent to consider them outliers.

```{r excluded-cases-table, echo = FALSE, cache = TRUE}
case_colnames <- c("Gender","Age","Attitude","Deep", "Strategic","Surface","Points")
knitr::kable(
  learning2014 %>% filter(id %in% c(35, 56, 71)) %>% select(-id),
  format="html", col.names = case_colnames, digits = 2, caption = "Outlier cases") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```
Looking the data, two unfortunate students have scored very low in the test, even though their motivation is strong. It might be that some external cause has affected their test performance as other low-scoring (points <=10) students somewhat differ from these cases. There is also one student whose attitude score is very low but who has achieved high points in tests.

```{r low-scoring-students, echo = FALSE, cache = TRUE}
case_colnames <- c("Gender","Age","Attitude","Deep", "Strategic","Surface","Points")
knitr::kable(
  learning2014 %>% filter(points <= 10) %>% select(-id),
  format="html", col.names = case_colnames, digits = 2, caption = "Outlier students") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```


