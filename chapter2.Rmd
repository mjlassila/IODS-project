```{r chapter-2-setup, cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
library(PerformanceAnalytics)
library(data.table)
library(tidyverse)
library(hrbrthemes)
library(ggcorrplot)
library(GoodmanKruskal)
library(lindia)
library(finalfit)
library(sigr)
library(papaja)
library(here)
library(kableExtra)
library(GGally)
```

# Week 2 â€“ Regression and model validation

```{r load-data, echo = FALSE, message= FALSE, cache = TRUE}

learning2014 <- fread(here("data/learning2014.csv"))

```

## Introduction


[see @vehkalahti2016]

Surface approach: memorizing without understanding, with a serious lack of personal engagement in the learning process. Intention of getting forward with minimium trouble

Deep approach: intention to maximize understanding with ta true commitment to learning. Strong need to engage in the actual content of the task.
STrategic approad: the ways students organize their studying.

ASSIST: the appproaches and study skills inventory for studets -survey (Tait, Etwishle, MacCune, 1008). Items were measured with a five point Likert skale (1=disagree, 5=agree)

Learning approach, 8 subscales, 4 items each.

1. Deep: seeking meaning, relating ideas, use of evidence
2. surface: lack of purpose, unrelated memorizing, syllabus-boundness
3. stratcgic: organized studying, time management

students achievement was measured by points in the exaams

## Exploratory analysis {#explorative-analysis}



```{r graphical-overview, cache = TRUE, echo = FALSE, fig.width = 12, fig.height = 4, message = FALSE}
# Show a graphical overview of the data and show summaries of the variables 
# in the data. Describe and interpret the outputs, 
# commenting on the distributions of the variables 
# and the relationships between them. (0-3 points)

ggpairs(
  learning2014,
  mapping = aes(col = gender, alpha = 0.3), 
        lower = list(combo = wrap("facethist", bins = 5))) + theme_minimal()
```

### Relationship of gender, attitude and strategic study approach

```{r graphical-overview-relationship, cache = TRUE, echo = FALSE}
ggplot(learning2014, aes(color = gender, x = attitude, y = points, size = stra)) + 
  geom_point() + 
  geom_smooth(method = "lm", fill = NA) +
  scale_size_continuous(range = c(0.05,2)) +
  theme_bw()
```

## Regression analysis

In building linear regression models, it is desirable to have high correlations between the prediction covariates and the response variable, but small correlations between the different prediction covariates. Large correlations between prediction variables leads to the [problem of collinearity in linear regression](https://en.wikipedia.org/wiki/Multicollinearity#Consequences_of_multicollinearity), eg. the model might overfit the data. Therefore, based on explorative analysis in [Fig 1](#explorative-analysis), two non-correlating variables where chosen an prediction covariates.

### Initial model

```{r initial-model-instruction, echo=FALSE, eval=FALSE}
# Choose three variables as explanatory variables and 
# fit a regression model where exam points is the target (dependent) variable.
```


```{r fit-attitude_stra_surf, cache = TRUE}
model_attitude_stra_surf <- lm(
  data = learning2014, 
  formula = points ~ attitude + stra + surf )

```

```{r model_attitude_stra_surf_summary, echo = FALSE, cache = TRUE}

# Show a summary of the fitted model and comment and interpret the results. 
# Explain and interpret the statistical test related to the model parameters.

summary_colnames <- c(
  "Predictor",
  "Estimate",
  "Confidence Interval (95%)",
  "t-statistic",
  "p-value")

model_attitude_stra_surf_summary <- apa_print(model_attitude_stra_surf)
knitr::kable(
  model_attitude_stra_surf_summary$table,
  format = "html", col.names = summary_colnames, caption = "Regression summary") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

`r render(wrapFTest(model_attitude_stra_surf, pSmallCutoff=0, format="markdown"))` Adjusted <i>R^2^</i> is `r round(summary(model_attitude_stra_surf)$adj.r.squared, 2)`.

### Final model

```{r final-model-instruction, echo = FALSE, eval = FALSE}
# If an explanatory variable in your model does not have 
# a statistically significant relationship with the target variable
# remove the variable from the model and fit the model again without it.
```

```{r fit-attitude_stra, cache = TRUE}
model_attitude_stra <- lm(
  data = learning2014, 
  formula = points ~ attitude + stra)
```

```{r model_model_attitude_stra_summary, echo = FALSE, cache = TRUE}
model_attitude_stra_summary <- apa_print(model_attitude_stra)
knitr::kable(
  model_attitude_stra_summary$table,
  format="html", col.names = summary_colnames, caption = "Regression summary") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```


For the final model `r render(wrapFTest(model_attitude_stra, pSmallCutoff=0, format="markdown"))` Adjusted <i>R^2^</i> is `r round(summary(model_attitude_stra)$adj.r.squared, 2)`.

## Regression diagnostics

Regression diagnostics plots help to observe, if the model meets the assumptions of linear regression:

* Linearity of residuals
* Independence of residuals
* Normal distribution of residuals
* Equal variance of residuals

```{r instructions, echo = FALSE, eval = FALSE, cache = TRUE}
# Produce the following diagnostic plots: 
# Residuals vs Fitted values
# Normal QQ-plot
# Residuals vs Leverage. 
# Explain the assumptions of the model and interpret the validity of 
# those assumptions based on the diagnostic plots. (0-3 points)
```




```{r residuals-fitted, echo = FALSE, cache = TRUE}
gg_resfitted(model_attitude_stra, scale.factor = 1) + theme_bw()
```


Residuals vs Fitted -plot shows if residuals have non-linear patterns. If the residuals have constant variance of errors, the plot scatters randomly without any distict patterns. We don't see patterns here, so we can conclude that the variance is constant and the assumption of equal variance of residuals is met.


```{r qqplot, echo = FALSE, cache = TRUE}
gg_qqplot(model_attitude_stra, scale.factor = 1) + theme_bw()

```

Normal Q-Q -plot shows if residuals are normally distributed. If the residuals deviate from the straight line it is an indication of non-normality. There are few outlying values in both ends of the line but majority of of the points line nicely and therefore we can conclude that the the residuals of the model are normally distributed.



```{r residual-leverage, echo = FALSE, cache = TRUE}
gg_resleverage(model_attitude_stra, method = "loess", se = FALSE, scale.factor = 1) + theme_bw()

```

Residual vs. leverage -plot helps to find influential cases. Not all outliers have leverage in linear regression analysis. Values to observe are at the upper right corner or at the lower right corner -- they are the ones which can have an effect on regression results. We don't see outlying values in these critical spots, so we can conclude that there is no influencial individual cases in the data.


## Extra - tweaking the model

Even though we just concluded that there's no influencial individual cases in the data, we can see from the plots that there are few outlying cases in non-critical spots of the Residual vs. leverage plot. What happens if we exclude them from the data? To do it, we have to single out the cases from the data using Cook's distance.

```{r cooks-distance, echo = FALSE, cache = TRUE}
gg_cooksd(
  model_attitude_stra,
  scale.factor = 1, show.threshold = FALSE) + 
  theme_bw()
```

Just for the sake of curiosity, let's investige how excluding two outlier cases (56 & 145) has an effect on our regression model. 

```{r fit-attitude_stra-excluded-cases, cache = TRUE}
learning2014$id <- 1:dim(learning2014)[1]
model_attitude_stra_excluded <- lm(
  data = learning2014 %>% filter(!id %in% c(35, 145)), 
  formula = points ~ attitude + stra)
```

```{r model_model_attitude_stra_excluded-summary, echo = FALSE, cache = TRUE}
model_attitude_stra_summary_excluded <- apa_print(model_attitude_stra_excluded)
knitr::kable(
  model_attitude_stra_summary_excluded$table,
  format="html", col.names = summary_colnames, caption = "Regression summary") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```


For the model created using data without cases 35 and 145, the `r render(wrapFTest(model_attitude_stra_excluded, pSmallCutoff=0, format="markdown"))` Adjusted <i>R^2^</i> is `r round(summary(model_attitude_stra_excluded)$adj.r.squared, 2)`.

The fit of the model seems to be marginally better when two outlying cases are excluded. Let's see from the data what kind of cases these are so that we could decide if it is prudent to consider them outliers.

```{r excluded-cases-table, echo = FALSE, cache = TRUE}
case_colnames <- c("Gender","Age","Attitude","Deep", "Strategic","Surface","Points")
knitr::kable(
  learning2014 %>% filter(id %in% c(35, 145)) %>% select(-id),
  format="html", col.names = case_colnames, digits = 2, caption = "Outlier cases") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```
Looking the data, these unfortunate students have scored very low in the test, even though their motivation is strong. It might be that some external cause has affected their test performance as other low-scoring (points <=10) students somewhat differ from these cases.

```{r low-scoring-students, echo = FALSE, cache = TRUE}
case_colnames <- c("Gender","Age","Attitude","Deep", "Strategic","Surface","Points")
knitr::kable(
  learning2014 %>% filter(points <= 10) %>% select(-id),
  format="html", col.names = case_colnames, digits = 2, caption = "Low-scoring students") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```


## References

